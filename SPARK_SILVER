from pyspark.sql.functions import col, regexp_replace, to_date

# Rutas de origen y destino
source_path = "abfss://EventoGuatemala_1@onelake.dfs.fabric.microsoft.com/LK_Bronce.Lakehouse/Tables/it"
dest_path   = "abfss://EventoGuatemala_1@onelake.dfs.fabric.microsoft.com/LK_Plata.Lakehouse/Tables/IT_PLATA"
             
# Leer la tabla Delta desde Bronce
df = spark.read.format("delta").load(source_path)

# Transformaciones para fase Plata:
# - Convertir fechas correctamente desde dd/MM/yyyy al tipo date
# - Reemplazar coma por punto en Latitud y Longitud y convertir a double
df_plata = df.withColumn("Fecha", to_date(col("Fecha"), "dd/MM/yyyy")) \
             .withColumn("FechadeResoluci√≥n", to_date(col("FechadeResoluci√≥n"), "dd/MM/yyyy")) \
             .withColumn("Latitud", regexp_replace("Latitud", ",", ".").cast("double")) \
             .withColumn("Longitud", regexp_replace("Longitud", ",", ".").cast("double"))

# Guardar en formato Delta con tipos correctos (Fecha como date)
df_plata.write.format("delta").mode("overwrite").save(dest_path)

print("‚úÖ Guardado exitoso")


from pyspark.sql.functions import col, count, row_number
from pyspark.sql.window import Window

# Ruta f√≠sica a la tabla en LK_Plata
ruta_tabla_plata = "abfss://EventoGuatemala_1@onelake.dfs.fabric.microsoft.com/LK_Plata.Lakehouse/Tables/IT_PLATA"

# 1. Leer directamente desde la ruta Delta
df = spark.read.format("delta").load(ruta_tabla_plata)

# 2. Detectar duplicados por IDdeTicket
df_duplicados = df.groupBy("IDdeTicket") \
                  .agg(count("*").alias("cantidad")) \
                  .filter(col("cantidad") > 1)

# 3. Mostrar duplicados encontrados
print("üîç Registros duplicados por IDdeTicket:")
df_duplicados.show(truncate=False)

# 4. Eliminar duplicados (quedarse solo con el primero por IDdeTicket)
window_spec = Window.partitionBy("IDdeTicket").orderBy("Fecha")  # Puedes ajustar el orden si lo prefieres

df_clean = df.withColumn("row_num", row_number().over(window_spec)) \
             .filter(col("row_num") == 1) \
             .drop("row_num")

# 5. Sobrescribir la tabla en LK_Plata sin duplicados
df_clean.write.format("delta").mode("overwrite").save(ruta_tabla_plata)

print("‚úÖ Duplicados eliminados y tabla IT_PLATA actualizada.")


from pyspark.sql.functions import col, trim, when
from functools import reduce


# Ruta f√≠sica a la tabla en LK_Plata
# ruta_tabla_plata = "abfss://EventoGuatemala_1@onelake.dfs.fabric.microsoft.com/LK_Plata.Lakehouse/Tables/IT_PLATA"

# 1. Cargar la tabla del Lakehouse
df = spark.read.format("delta").load("abfss://EventoGuatemala_1@onelake.dfs.fabric.microsoft.com/LK_Plata.Lakehouse/Tables/IT_PLATA")

# 2. Columnas a revisar
columnas_objetivo = ["Tipo", "Cola", "Prioridad", "EtiquetaPrimaria", "EtiquetaSecundaria", "Categor√≠a"]

# 3. Filtrar registros con campos vac√≠os o nulos
condicion_vacios = reduce(
    lambda acc, c: acc | (trim(col(c)) == "") | col(c).isNull(),
    columnas_objetivo[1:],
    (trim(col(columnas_objetivo[0])) == "") | col(columnas_objetivo[0]).isNull()
)

df_vacios = df.filter(condicion_vacios)

# 4. Mostrar registros con problemas
print("üîç Registros con campos vac√≠os o nulos:")
df_vacios.select(columnas_objetivo).show(truncate=False)

# 5. Reemplazar vac√≠os o nulos con "Desconocido"
for c in columnas_objetivo:
    df = df.withColumn(
        c,
        when((trim(col(c)) == "") | col(c).isNull(), "Desconocido").otherwise(col(c))
    )

# 6. Guardar sobreescribiendo tabla Delta
df.write.format("delta").mode("overwrite").save("abfss://EventoGuatemala_1@onelake.dfs.fabric.microsoft.com/LK_Plata.Lakehouse/Tables/IT_PLATA")

print("‚úÖ Reemplazo completado y tabla IT_PLATA actualizada.")
